x = fct_reorder(album_name, total_streams),
y = total_streams,
fill = total_streams)) +
geom_col() +
coord_flip() +
labs(
title = "Total Streams by Album in Billions",
x = NULL,
y = NULL
) +
scale_fill_gradient(
low = "violet",
high = "darkviolet"
)+
theme(legend.position = "None")
ggsave(
"Total Streams by Album in Billions.png",
plot = T_bar_album_streams,
width = 16,
height = 10,
units = "cm",
dpi = 300
)
T_bar_album_streams <- album_totals |>
ggplot(aes(
x = fct_reorder(album_name, total_streams),
y = total_streams,
fill = total_streams)) +
geom_col() +
coord_flip() +
labs(
title = "Total Streams by Album in Billions",
x = NULL,
y = NULL
) +
scale_fill_gradient(
low = "violet",
high = "darkviolet"
)+
theme(legend.position = "None")
ggsave(
"Total Streams by Album in Billions.png",
plot = T_bar_album_streams,
width = 16,
height = 10,
units = "cm",
dpi = 300
)
#| echo: true
#| output: false
library(tidyverse)
library(vroom)
taylor <- vroom("clean_taylor_data.csv")
taylor|>
group_by(key_mode)|>
summarise(counts = count(key_mode))|>
slice_max(total_streams, n = 5)
taylor |>
count(key_mode) |>
slice_max(n, n = 5)
col(taylor)
colnames(taylor)
model_data <- taylor|>
select(streams_by_billion,daceability,energy,key,loudness,mode,speechiness,acousticness,instrumentalness,liveness,valence,tempo,time_signature,duration_ms)
model_data <- taylor|>
select(streams_by_billion,danceability,energy,key,loudness,mode,speechiness,acousticness,instrumentalness,liveness,valence,tempo,time_signature,duration_ms)
#| echo: true
#| output: false
library(tidyverse)
library(vroom)
library(glmnet)
taylor <- vroom("clean_taylor_data.csv")
nrow(taylor)
taylor|>
filter(streams == max(streams))|>
select(track_name, streams)
taylor|>
group_by(album_name)|>
summarise(total_streams = sum(streams_by_billion))|>
slice_max(total_streams, n = 5)
taylor |>
count(key_mode) |>
slice_max(n, n = 5)
model_data <- taylor|>
select(streams_by_billion,danceability,energy,key,loudness,mode,speechiness,acousticness,instrumentalness,liveness,valence,tempo,time_signature,duration_ms)
# Matrix for glmnet
X <- model.matrix(streams_by_billion ~ ., data = model_data)[, -1]
y <- model_data$streams_by_billion
# alpha = 0.5 for Elastic Net
cv_enet <- cv.glmnet(X, y, type.measure = "mse", alpha = 0.5, nfolds = 10)
#| echo: true
#| output: false
library(tidyverse)
library(vroom)
library(glmnet)
taylor <- vroom("clean_taylor_data.csv")
nrow(taylor)
taylor|>
filter(streams == max(streams))|>
select(track_name, streams)
taylor|>
group_by(album_name)|>
summarise(total_streams = sum(streams_by_billion))|>
slice_max(total_streams, n = 5)
taylor |>
count(key_mode) |>
slice_max(n, n = 5)
model_data <- taylor|>
select(streams_by_billion,danceability,energy,key,loudness,mode,speechiness,acousticness,instrumentalness,liveness,valence,tempo,time_signature,duration_ms)
# Matrix for glmnet
# Keep only complete rows
model_data_clean <- model_data |> drop_na()
# Predictor matrix and response
X <- model.matrix(streams_by_billion ~ ., data = model_data_clean)[, -1]
y <- model_data_clean$streams_by_billion
# alpha = 0.5 for Elastic Net
cv_enet <- cv.glmnet(X, y, type.measure = "mse", alpha = 0.5, nfolds = 10)
# Best lambda
cv_enet$lambda.min
cv_enet$lambda.1se
# Fit final model
enet_model <- glmnet(X, y, alpha = 0.5, lambda = cv_enet$lambda.min)
# Coefficients
coef(enet_model)
# Predicted vs Actual
preds <- predict(enet_model, newx = X)
plot(y, preds,
xlab = "Actual Streams (Billion)",
ylab = "Predicted Streams (Billion)",
main = "Predicted vs Actual")
abline(0,1, col = "red", lwd = 2)
# Residuals
residuals <- y - as.vector(preds)
hist(residuals, breaks = 30, main = "Residuals", xlab = "Residuals")
qqnorm(residuals); qqline(residuals, col = "red")
cor_matrix <- cor(model_data[, -1], use = "pairwise.complete.obs")
corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.8, tl.col = "black",
addCoef.col = "black")
#| echo: true
#| output: false
library(tidyverse)
library(vroom)
library(glmnet)
taylor <- vroom("clean_taylor_data.csv")
nrow(taylor)
taylor|>
filter(streams == max(streams))|>
select(track_name, streams)
taylor|>
group_by(album_name)|>
summarise(total_streams = sum(streams_by_billion))|>
slice_max(total_streams, n = 5)
taylor |>
count(key_mode) |>
slice_max(n, n = 5)
model_data <- taylor|>
select(streams_by_billion,danceability,energy,key,loudness,mode,speechiness,acousticness,instrumentalness,liveness,valence,tempo,time_signature,duration_ms)
# Matrix for glmnet
# Keep only complete rows
model_data_clean <- model_data |> drop_na()
# Predictor matrix and response
X <- model.matrix(streams_by_billion ~ ., data = model_data_clean)[, -1]
y <- model_data_clean$streams_by_billion
# alpha = 0.5 for Elastic Net
cv_enet <- cv.glmnet(X, y, type.measure = "mse", alpha = 0.5, nfolds = 10)
# Best lambda
cv_enet$lambda.min
cv_enet$lambda.1se
# Fit final model
enet_model <- glmnet(X, y, alpha = 0.5, lambda = cv_enet$lambda.min)
# Coefficients
coef(enet_model)
# Predicted vs Actual
preds <- predict(enet_model, newx = X)
plot(y, preds,
xlab = "Actual Streams (Billion)",
ylab = "Predicted Streams (Billion)",
main = "Predicted vs Actual")
abline(0,1, col = "red", lwd = 2)
# Residuals
residuals <- y - as.vector(preds)
hist(residuals, breaks = 30, main = "Residuals", xlab = "Residuals")
qqnorm(residuals); qqline(residuals, col = "red")
cor_matrix <- cor(model_data[, -1], use = "pairwise.complete.obs")
corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.8, tl.col = "black",
addCoef.col = "black")
coef_df <- as.data.frame(as.matrix(coef(enet_model)))
coef_df$feature <- rownames(coef_df)
names(coef_df)[1] <- "coefficient"
coef_df <- coef_df[-1,] # remove intercept
ggplot(coef_df, aes(x = reorder(feature, abs(coefficient)), y = abs(coefficient))) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Feature Importance from Elastic Net", x = NULL, y = "Absolute Coefficient")
# -----------------------------
# Load Data and Packages
# -----------------------------
library(tidyverse)
library(glmnet)
library(corrplot)
library(patchwork)
set.seed(201)
taylor <- vroom("clean_taylor_data.csv")
# -----------------------------
# Prepare data
# -----------------------------
# Drop rows with any NA
model_data <- taylor|>
select(streams_by_billion,danceability,
energy,key,loudness,mode,speechiness,
acousticness,instrumentalness,liveness,
valence,tempo,time_signature,duration_ms)
model_data_clean <- model_data |> drop_na()
# Predictor matrix
X <- model.matrix(streams_by_billion ~ ., data = model_data_clean)[, -1]
# Log-transform the response
y_log <- log(model_data_clean$streams_by_billion + 1e-6)  # add tiny constant to avoid log(0)
# -----------------------------
# Elastic Net with cross-validation
# -----------------------------
cv_enet_log <- cv.glmnet(
X, y_log,
type.measure = "mse",
alpha = 0.5,   # Elastic Net
nfolds = 10
)
# Best lambda
cv_enet_log$lambda.min
cv_enet_log$lambda.1se
# Fit final model
enet_model_log <- glmnet(
X, y_log,
alpha = 0.5,
lambda = cv_enet_log$lambda.min
)
# -----------------------------
# Predicted vs Actual (log scale)
# -----------------------------
preds_log <- predict(enet_model_log, newx = X)
plot(y_log, preds_log,
xlab = "Actual log(Streams)",
ylab = "Predicted log(Streams)",
main = "Predicted vs Actual (log-transformed)")
abline(0,1,col="red",lwd=2)
# -----------------------------
# Residuals
# -----------------------------
residuals_log <- y_log - as.vector(preds_log)
hist(residuals_log, breaks=30,
main="Residuals (log-transformed)",
xlab="Residuals")
qqnorm(residuals_log); qqline(residuals_log, col="red")
# -----------------------------
# Correlation plot of predictors
# -----------------------------
cor_matrix <- cor(model_data_clean[, -1], use = "pairwise.complete.obs")
corrplot(cor_matrix, method = "color", type = "upper",
tl.cex = 0.8, tl.col = "black",
addCoef.col = "black")
# -----------------------------
# Feature importance (coefficients)
# -----------------------------
coef_df <- as.data.frame(as.matrix(coef(enet_model_log)))
coef_df$feature <- rownames(coef_df)
names(coef_df)[1] <- "coefficient"
coef_df <- coef_df[-1,] # remove intercept
ggplot(coef_df, aes(x = reorder(feature, abs(coefficient)), y = abs(coefficient))) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Feature Importance (Elastic Net, log response)",
x = NULL,
y = "Absolute Coefficient")
# Predictor matrix
X <- model.matrix(streams_by_billion ~ ., data = model_data_clean)[, -1]
# -----------------------------
# Load Data and Packages
# -----------------------------
library(tidyverse)
library(glmnet)
library(corrplot)
library(patchwork)
set.seed(201)
taylor <- vroom("clean_taylor_data.csv")
# -----------------------------
# Prepare data
# -----------------------------
# Drop rows with any NA
model_data <- taylor|>
select(streams_by_billion,danceability,
energy,key,loudness,mode,speechiness,
acousticness,instrumentalness,liveness,
valence,tempo,time_signature,duration_ms)
model_data_clean <- model_data |> drop_na()
# Predictor matrix
X <- model.matrix(streams_by_billion ~ ., data = model_data_clean)[, -1]
# Log-transform the response
y_log <- log(model_data_clean$streams_by_billion + 1e-6)  # avoid log(0)
# -----------------------------
# Tune Elastic Net
# -----------------------------
# Try multiple alphas to reduce overshrinking
alphas <- c(0.1, 0.3, 0.5, 0.7, 0.9)
cv_results <- list()
for(a in alphas){
cv_results[[paste0("alpha_", a)]] <- cv.glmnet(X, y_log, alpha = a, nfolds = 10, type.measure = "mse")
}
# Pick alpha with lowest CV error
cv_mses <- sapply(cv_results, function(x) min(x$cvm))
best_alpha <- alphas[which.min(cv_mses)]
cv_best <- cv_results[[paste0("alpha_", best_alpha)]]
cat("Best alpha:", best_alpha, "\n")
cat("Lambda.min:", cv_best$lambda.min, "\n")
cat("Lambda.1se:", cv_best$lambda.1se, "\n")
# Fit final Elastic Net model
enet_model <- glmnet(X, y_log, alpha = best_alpha, lambda = cv_best$lambda.1se)
# -----------------------------
# Predicted vs Actual
# -----------------------------
preds_log <- predict(enet_model, newx = X)
plot(y_log, preds_log,
xlab = "Actual log(Streams)",
ylab = "Predicted log(Streams)",
main = "Predicted vs Actual (log-transformed)")
abline(0,1,col="red",lwd=2)
# -----------------------------
# Residuals
# -----------------------------
residuals_log <- y_log - as.vector(preds_log)
hist(residuals_log, breaks=30, main="Residuals (log-transformed)", xlab="Residuals")
qqnorm(residuals_log); qqline(residuals_log, col="red")
# -----------------------------
# Correlation plot of predictors
# -----------------------------
cor_matrix <- cor(model_data_clean[, -1], use = "pairwise.complete.obs")
corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.8, tl.col = "black", addCoef.col = "black")
# -----------------------------
# Feature importance
# -----------------------------
coef_df <- as.data.frame(as.matrix(coef(enet_model)))
coef_df$feature <- rownames(coef_df)
names(coef_df)[1] <- "coefficient"
coef_df <- coef_df[-1,] # remove intercept
ggplot(coef_df, aes(x = reorder(feature, abs(coefficient)), y = abs(coefficient))) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = paste0("Feature Importance (Elastic Net, log response, alpha=", best_alpha, ")"),
x = NULL, y = "Absolute Coefficient")
# -----------------------------
# Load Data and Packages
# -----------------------------
library(tidyverse)
library(glmnet)
library(corrplot)
library(patchwork)
library(vroom)
set.seed(201)
taylor <- vroom("clean_taylor_data.csv")
# -----------------------------
# Prepare data
# -----------------------------
model_data <- taylor |>
select(streams_by_billion, danceability, energy, key, loudness, mode,
speechiness, acousticness, instrumentalness, liveness, valence,
tempo, time_signature, duration_ms) |>
drop_na()
# Predictor matrix
X <- model.matrix(streams_by_billion ~ ., data = model_data)[, -1]
# Log-transform the response
y_log <- log(model_data$streams_by_billion + 1e-6)
# -----------------------------
# Elastic Net (ridge-heavy) with Cross-Validation
# -----------------------------
# Try a low alpha to keep most predictors
alpha_ridgeish <- 0.1
cv_enet <- cv.glmnet(X, y_log, alpha = alpha_ridgeish, nfolds = 10, type.measure = "mse")
cat("Alpha used:", alpha_ridgeish, "\n")
cat("Lambda.min:", cv_enet$lambda.min, "\n")
cat("Lambda.1se:", cv_enet$lambda.1se, "\n")
# Fit final model using lambda.min for less shrinkage
enet_model <- glmnet(X, y_log, alpha = alpha_ridgeish, lambda = cv_enet$lambda.min)
# -----------------------------
# Predicted vs Actual
# -----------------------------
preds_log <- predict(enet_model, newx = X)
plot(y_log, preds_log,
xlab = "Actual log(Streams)",
ylab = "Predicted log(Streams)",
main = "Predicted vs Actual (log-transformed)")
abline(0,1,col="red",lwd=2)
# -----------------------------
# Residuals
# -----------------------------
residuals_log <- y_log - as.vector(preds_log)
hist(residuals_log, breaks = 30, main = "Residuals (log-transformed)", xlab = "Residuals")
qqnorm(residuals_log); qqline(residuals_log, col = "red")
# -----------------------------
# Correlation of predictors
# -----------------------------
cor_matrix <- cor(model_data[, -1], use = "pairwise.complete.obs")
corrplot(cor_matrix, method = "color", type = "upper",
tl.cex = 0.8, tl.col = "black", addCoef.col = "black")
# -----------------------------
# Feature importance (coefficients)
# -----------------------------
coef_df <- as.data.frame(as.matrix(coef(enet_model)))
coef_df$feature <- rownames(coef_df)
names(coef_df)[1] <- "coefficient"
coef_df <- coef_df[-1,]  # remove intercept
ggplot(coef_df, aes(x = reorder(feature, abs(coefficient)), y = abs(coefficient))) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = paste0("Feature Importance (Elastic Net, log response, alpha=", alpha_ridgeish, ")"),
x = NULL, y = "Absolute Coefficient")
# -----------------------------
# Load Data and Packages
# -----------------------------
library(tidyverse)
library(glmnet)
library(corrplot)
library(patchwork)
library(vroom)
set.seed(201)
taylor <- vroom("clean_taylor_data.csv")
# -----------------------------
# Prepare data
# -----------------------------
model_data <- taylor |>
select(streams, danceability, energy, key, loudness, mode,
speechiness, acousticness, instrumentalness, liveness, valence,
tempo, time_signature, duration_ms) |>
drop_na()
# Predictor matrix
X <- model.matrix(streams ~ ., data = model_data)[, -1]
# Log-transform the response
y_log <- log(model_data$streams_by_billion + 1e-6)
# -----------------------------
# Elastic Net (ridge-heavy) with Cross-Validation
# -----------------------------
# Try a low alpha to keep most predictors
alpha_ridgeish <- 0.1
cv_enet <- cv.glmnet(X, y_log, alpha = alpha_ridgeish, nfolds = 10, type.measure = "mse")
# -----------------------------
# Load Data and Packages
# -----------------------------
library(tidyverse)
library(glmnet)
library(corrplot)
library(patchwork)
library(vroom)
set.seed(201)
taylor <- vroom("clean_taylor_data.csv")
# -----------------------------
# Prepare data
# -----------------------------
model_data <- taylor |>
select(streams, danceability, energy, key, loudness, mode,
speechiness, acousticness, instrumentalness, liveness, valence,
tempo, time_signature, duration_ms) |>
drop_na()
# Predictor matrix
X <- model.matrix(streams ~ ., data = model_data)[, -1]
# Log-transform the response
y_log <- log(model_data$streams + 1e-6)
# -----------------------------
# Elastic Net (ridge-heavy) with Cross-Validation
# -----------------------------
alpha_ridgeish <- 0.1
cv_enet <- cv.glmnet(X, y_log, alpha = alpha_ridgeish, nfolds = 10, type.measure = "mse")
cat("Alpha used:", alpha_ridgeish, "\n")
cat("Lambda.min:", cv_enet$lambda.min, "\n")
cat("Lambda.1se:", cv_enet$lambda.1se, "\n")
# Fit final model using lambda.min for less shrinkage
enet_model <- glmnet(X, y_log, alpha = alpha_ridgeish, lambda = cv_enet$lambda.min)
# -----------------------------
# Predicted vs Actual
# -----------------------------
preds_log <- predict(enet_model, newx = X)
plot(y_log, preds_log,
xlab = "Actual log(Streams)",
ylab = "Predicted log(Streams)",
main = "Predicted vs Actual (log-transformed)")
abline(0,1,col="red",lwd=2)
# -----------------------------
# Residuals
# -----------------------------
residuals_log <- y_log - as.vector(preds_log)
hist(residuals_log, breaks = 30, main = "Residuals (log-transformed)", xlab = "Residuals")
qqnorm(residuals_log); qqline(residuals_log, col = "red")
plot(preds_log, residuals_log,
xlab = "Fitted log(Streams)",
ylab = "Residuals",
main = "Residuals vs Fitted")
abline(h = 0, col = "red")
# -----------------------------
# Correlation of predictors
# -----------------------------
cor_matrix <- cor(model_data[, -1], use = "pairwise.complete.obs")
corrplot(cor_matrix, method = "color", type = "upper",
tl.cex = 0.8, tl.col = "black", addCoef.col = "black")
# -----------------------------
# Feature importance (coefficients)
# -----------------------------
coef_df <- as.data.frame(as.matrix(coef(enet_model)))
coef_df$feature <- rownames(coef_df)
names(coef_df)[1] <- "coefficient"
coef_df <- coef_df[-1,]  # remove intercept
ggplot(coef_df, aes(x = reorder(feature, abs(coefficient)), y = abs(coefficient))) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = paste0("Feature Importance (Elastic Net, log response, alpha=", alpha_ridgeish, ")"),
x = NULL, y = "Absolute Coefficient")
reticulate::repl_python()
