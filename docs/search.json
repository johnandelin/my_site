[
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "Random Forest Tutorial",
    "section": "",
    "text": "Random Forests are a powerful ensemble machine learning algorithm for regression and classification. They combine many decision trees to produce stable predictions and automatically handle feature interactions and non-normal target distributions.\nIn this tutorial, we will predict the number of days patients are sick using a simulated dataset with 5 predictors:\n\nage (numeric)\n\nsymptom_severity (numeric, 1–10)\n\ntreatment_type (categorical, A/B/C)\n\npre_existing_condition (categorical: none/mild/severe)\n\nexercise_level (numeric, hours per week)\n\nWe will go over how to:\n\nSimulate a data set.\n\nBuild a Random Forest regression pipeline.\n\nPerform cross-validation and hyperparameter tuning.\n\nMake predictions and evaluate model performance.\n\nRandom Forests can be a strong choice with health data because health outcomes often have interactions (e.g., age × pre-existing condition) and the number of days sick may not follow a normal distribution."
  },
  {
    "objectID": "tutorial.html#simulate-dataset",
    "href": "tutorial.html#simulate-dataset",
    "title": "Random Forest Tutorial",
    "section": "Simulate dataset",
    "text": "Simulate dataset\n\nnp.random.seed(42)\nn = 2000\ndf = pd.DataFrame({\n    \"age\": np.random.randint(20, 80, n),\n    \"symptom_severity\": np.random.randint(1, 11, n),\n    \"treatment_type\": np.random.choice([\"A\",\"B\",\"C\"], n),\n    \"pre_existing_condition\": np.random.choice([\"none\",\"mild\",\"severe\"], n),\n    \"exercise_level\": np.random.randint(0, 8, n)  # hours per week\n})\n\ndf[\"days_sick\"] = (2 + 0.3*df[\"age\"]/10 + 1.5*df[\"symptom_severity\"]\n                   - 0.2*df[\"exercise_level\"]\n                   + df[\"treatment_type\"].map({\"A\":0,\"B\":-1,\"C\":1})\n                   + df[\"pre_existing_condition\"].map({\"none\":0,\"mild\":1,\"severe\":3})\n                   + np.random.randn(n)*2)\n\ndf.head()\n\n\n\n\n\n\n\n\nage\nsymptom_severity\ntreatment_type\npre_existing_condition\nexercise_level\ndays_sick\n\n\n\n\n0\n58\n9\nC\nmild\n1\n17.962330\n\n\n1\n71\n1\nB\nsevere\n4\n3.634408\n\n\n2\n48\n9\nC\nmild\n7\n20.471153\n\n\n3\n34\n8\nB\nmild\n6\n12.864343\n\n\n4\n62\n4\nA\nmild\n7\n10.038025\n\n\n\n\n\n\n\nThis code is creating a simulated dataset with 2000 patients. It generates random values for age, symptom severity, treatment type, pre-existing conditions, and weekly exercise hours. Then it calculates a synthetic outcome, days_sick, as a combination of these factors plus some random noise, simulating how different variables might influence recovery time. This allows the target (days_sick) to increases with age, symptom severity, and worse pre-existing conditions, and decreases with exercise. Random noise simulates real-world variability."
  },
  {
    "objectID": "tutorial.html#building-the-pipeline",
    "href": "tutorial.html#building-the-pipeline",
    "title": "Random Forest Tutorial",
    "section": "Building the pipeline",
    "text": "Building the pipeline\nPipelines are useful because they combine preprocessing and modeling steps into a single workflow, making the code cleaner and easier to manage. They ensure that the same transformations are applied consistently to training and test data. Pipelines also make it easy to integrate with tools like RandomizedSearchCV for hyperparameter tuning without manually repeating steps.\n\nRF_pipeline = Pipeline([\n    (\"preprocess\", RF_preprocessor),\n    (\"model\", RandomForestRegressor(n_estimators=500, random_state=42))\n])\n\nThis creates RF_pipeline with two steps. First, \"preprocess\" applies RF_preprocessor to encode categorical features while keeping numeric features unchanged. Then, \"model\" fits a RandomForestRegressor with 500 trees. The n_estimators=500 sets the number of trees, where more trees can improve accuracy but take longer to compute, and random_state=42 ensures the randomness in building the trees is reproducible so you get the same results each time."
  },
  {
    "objectID": "tutorial.html#hyperparameter-tuning",
    "href": "tutorial.html#hyperparameter-tuning",
    "title": "Random Forest Tutorial",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\nTuning is important because the performance of a Random Forest depends on its hyperparameters, like the number of trees, tree depth, and minimum samples per leaf. Choosing the right values helps the model fit the data well without overfitting or underfitting. Using tools like RandomizedSearchCV with cross-validation allows us to systematically find hyperparameters that give the best predictive performance on new data.\n\nparam_dist = {\n    \"model__n_estimators\": randint(300,700),\n    \"model__max_depth\": randint(5,20),\n    \"model__min_samples_split\": randint(2,10),\n    \"model__min_samples_leaf\": randint(1,5),\n    \"model__max_features\": [\"sqrt\",\"log2\",None]\n}\n\nThis defines a dictionary param_dist specifying the range of hyperparameters to try when tuning the Random Forest model. Each key corresponds to a model parameter: n_estimators sets the number of trees, max_depth limits tree depth, min_samples_split and min_samples_leaf control how nodes are split, and max_features determines how many features are considered at each split. The randint function specifies a range of integer values to sample during randomized search, while max_features uses a fixed set of options.\nCross-validation is a technique used to evaluate how well a model generalizes to new data. It splits the training data into multiple folds, trains the model on some folds, and validates it on the remaining fold, repeating this process across all folds. This helps ensure that the hyperparameters we choose and the model we train perform well on unseen data, reducing the risk of overfitting.\n\nRF_search = RandomizedSearchCV(\n    RF_pipeline,\n    param_distributions=param_dist,\n    n_iter=5,\n    cv=5,\n    scoring=\"neg_mean_squared_error\",\n    n_jobs=1,\n    random_state=42\n)\n\nmodel = RF_search.fit(X_train, y_train)\n\nThis sets up a RandomizedSearchCV object called RF_search to tune the Random Forest pipeline. It tries 5 random combinations of the hyperparameters defined in param_dist, using 5-fold cross-validation to evaluate each combination. The model is scored using negative mean squared error (neg_mean_squared_error), runs in parallel on 1 CPU cores (n_jobs=1), and uses a fixed random_state=42. Finally, RF_search.fit(X_train, y_train) trains the model on the training data and finds the best hyperparameter combination."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a Statistics student at Brigham Young University with an emphasis in Data Science. I am interested in using data to solve real-world problems, particularly in health, and public safety. Before Statistics I was studying microbiology and was involvd in research.\nThrough coursework, research, and personal projects, I have developed a strong foundation in statistical modeling, machine learning, and data science tools across R and Python ecosystems."
  },
  {
    "objectID": "about.html#brigham-young-university-provo-ut",
    "href": "about.html#brigham-young-university-provo-ut",
    "title": "About Me",
    "section": "Brigham Young University — Provo, UT",
    "text": "Brigham Young University — Provo, UT\nBachelor of Science in Statistics (Data Science)\nExpected Graduation: April 2027\nGPA: 3.4\nRelevant Coursework\n\nAnalysis of Variance\n\nProbability & Statistical Inference\n\nLinear Regression Modeling\n\nPredictive Analytics & Machine Learning\n\nData Visualization (Tableau)\n\nData Science Ecosystems (SQL, Python, Linux, AWS)\n\nCalculus and Linear Algebra\nIntroduction to Computer Science (python)\nData Structures (C++)\n\nInvolvement\n\nMember, Statistics Club\n\nStatistics Department Teaching Assistant"
  },
  {
    "objectID": "about.html#statistics-department-teaching-assistant",
    "href": "about.html#statistics-department-teaching-assistant",
    "title": "About Me",
    "section": "Statistics Department — Teaching Assistant",
    "text": "Statistics Department — Teaching Assistant\nBrigham Young University | Jan 2025 – Present\n\nAssist students with probability, statistical modeling, and programming in R and Python\n\nProvide one-on-one conceptual and coding support\n\nSupport course logistics using Canvas and BYU Learning Suite\n\nStrengthened communication skills through technical instruction"
  },
  {
    "objectID": "about.html#risk-management-department-technician",
    "href": "about.html#risk-management-department-technician",
    "title": "About Me",
    "section": "Risk Management Department — Technician",
    "text": "Risk Management Department — Technician\nBrigham Young University | Aug 2024 – Dec 2024\n\nInspected university laboratories for OSHA compliance\n\nConducted HAZCAT testing on unknown substances\n\nMaintained detailed safety and compliance records\n\nDemonstrated strong attention to detail and data accuracy"
  },
  {
    "objectID": "about.html#summit-nutritional-laboratories-physical-chemistry-qc-technician",
    "href": "about.html#summit-nutritional-laboratories-physical-chemistry-qc-technician",
    "title": "About Me",
    "section": "Summit Nutritional Laboratories — Physical Chemistry QC Technician",
    "text": "Summit Nutritional Laboratories — Physical Chemistry QC Technician\nSpanish Fork, UT | Dec 2022 – Sep 2023\n\nPerformed quality control testing on nutraceutical products\n\nPrepared samples for inductively coupled plasma (ICP) analysis\n\nFollowed strict laboratory protocols and documentation standards"
  },
  {
    "objectID": "about.html#fabric-shrinkage-analysis",
    "href": "about.html#fabric-shrinkage-analysis",
    "title": "About Me",
    "section": "Fabric Shrinkage Analysis",
    "text": "Fabric Shrinkage Analysis\n\nCollected and analyzed experimental data across washing and drying cycles\n\nApplied ANOVA and ANCOVA techniques in R\n\nProduced data visualizations and statistical summaries"
  },
  {
    "objectID": "about.html#kaggle-forest-cover-prediction",
    "href": "about.html#kaggle-forest-cover-prediction",
    "title": "About Me",
    "section": "Kaggle Forest Cover Prediction",
    "text": "Kaggle Forest Cover Prediction\n\nBuilt a machine learning pipeline in R to predict forest cover types\n\nPerformed feature engineering, preprocessing, and normalization\n\nDeveloped and ensembled XGBoost, LightGBM, and Random Forest models\n\nAchieved strong predictive performance on held-out test data"
  },
  {
    "objectID": "about.html#yersina-ruckeri-pathogenesis-research",
    "href": "about.html#yersina-ruckeri-pathogenesis-research",
    "title": "About Me",
    "section": "Yersina ruckeri Pathogenesis Research",
    "text": "Yersina ruckeri Pathogenesis Research\n\nAwarded an undergraduate award to research Y. ruckeri Pathogenesis.\nStudied and ran tests on whether the cdsAB operon and I-Cysteine uptake systems relate to pathogenesis\nLearned Microbiology lab techniques."
  },
  {
    "objectID": "about.html#programming",
    "href": "about.html#programming",
    "title": "About Me",
    "section": "Programming",
    "text": "Programming\nR, Python, SQL, Linux, C++"
  },
  {
    "objectID": "about.html#data-visualization",
    "href": "about.html#data-visualization",
    "title": "About Me",
    "section": "Data Visualization",
    "text": "Data Visualization\nTableau, ggplot2, Matplotlib"
  },
  {
    "objectID": "about.html#statistical-methods",
    "href": "about.html#statistical-methods",
    "title": "About Me",
    "section": "Statistical Methods",
    "text": "Statistical Methods\nANOVA, ANCOVA, Linear Regression, Hypothesis Testing, Predictive Modeling"
  },
  {
    "objectID": "about.html#machine-learning",
    "href": "about.html#machine-learning",
    "title": "About Me",
    "section": "Machine Learning",
    "text": "Machine Learning\nFeature engineering, ensemble methods, text and time-series modeling"
  },
  {
    "objectID": "about.html#laboratory-scientific-skills",
    "href": "about.html#laboratory-scientific-skills",
    "title": "About Me",
    "section": "Laboratory & Scientific Skills",
    "text": "Laboratory & Scientific Skills\nAseptic techniques, PCR, microbial analysis, scientific data management"
  },
  {
    "objectID": "about.html#professional-skills",
    "href": "about.html#professional-skills",
    "title": "About Me",
    "section": "Professional Skills",
    "text": "Professional Skills\nTechnical communication, teamwork, analytical thinking, problem-solving"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "John Andelin",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]